{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checkmate\n",
    "from checkmate import BestCheckpointSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, threshold=73):\n",
    "    titles = {i: i.lower() for i in data.columns}\n",
    "    return (data.rename(columns=titles).query('expected <= @threshold').copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_pad(data, size, steps):\n",
    "    arr = []\n",
    "    for i in data:\n",
    "        _ = i.copy()\n",
    "        _.resize(steps, size)\n",
    "        arr.append(_)\n",
    "    arr = np.array(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training - 20 days of each month\n",
    "test - remaining days\n",
    "Both sets are independent\n",
    "Calendar time and location information omitted:\n",
    "    impossible to construct a local validation holdout subset independent from the rest of the training set\n",
    "    no way to ensure that any two gauge readings are not correlated in time/space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zf1 = zipfile.ZipFile('../../train.zip')\n",
    "#zf2 = zipfile.ZipFile('../../test.zip')\n",
    "#train = pd.read_csv(zf1.open(zipfile.ZipFile.namelist(zf1)[0]))\n",
    "#test = pd.read_csv(zf2.open(zipfile.ZipFile.namelist(zf2)[0]))\n",
    "train = pd.read_csv('../../train.csv', nrows=60000).pipe(remove_outliers)\n",
    "test = pd.read_csv('../../test.csv', nrows=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>minutes_past</th>\n",
       "      <th>radardist_km</th>\n",
       "      <th>ref</th>\n",
       "      <th>ref_5x5_10th</th>\n",
       "      <th>ref_5x5_50th</th>\n",
       "      <th>ref_5x5_90th</th>\n",
       "      <th>refcomposite</th>\n",
       "      <th>refcomposite_5x5_10th</th>\n",
       "      <th>refcomposite_5x5_50th</th>\n",
       "      <th>...</th>\n",
       "      <th>rhohv_5x5_90th</th>\n",
       "      <th>zdr</th>\n",
       "      <th>zdr_5x5_10th</th>\n",
       "      <th>zdr_5x5_50th</th>\n",
       "      <th>zdr_5x5_90th</th>\n",
       "      <th>kdp</th>\n",
       "      <th>kdp_5x5_10th</th>\n",
       "      <th>kdp_5x5_50th</th>\n",
       "      <th>kdp_5x5_90th</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  minutes_past  radardist_km  ref  ref_5x5_10th  ref_5x5_50th  \\\n",
       "0   1             3          10.0  NaN           NaN           NaN   \n",
       "1   1            16          10.0  NaN           NaN           NaN   \n",
       "2   1            25          10.0  NaN           NaN           NaN   \n",
       "3   1            35          10.0  NaN           NaN           NaN   \n",
       "4   1            45          10.0  NaN           NaN           NaN   \n",
       "\n",
       "   ref_5x5_90th  refcomposite  refcomposite_5x5_10th  refcomposite_5x5_50th  \\\n",
       "0           NaN           NaN                    NaN                    NaN   \n",
       "1           NaN           NaN                    NaN                    NaN   \n",
       "2           NaN           NaN                    NaN                    NaN   \n",
       "3           NaN           NaN                    NaN                    NaN   \n",
       "4           NaN           NaN                    NaN                    NaN   \n",
       "\n",
       "   ...  rhohv_5x5_90th  zdr  zdr_5x5_10th  zdr_5x5_50th  zdr_5x5_90th  kdp  \\\n",
       "0  ...             NaN  NaN           NaN           NaN           NaN  NaN   \n",
       "1  ...             NaN  NaN           NaN           NaN           NaN  NaN   \n",
       "2  ...             NaN  NaN           NaN           NaN           NaN  NaN   \n",
       "3  ...             NaN  NaN           NaN           NaN           NaN  NaN   \n",
       "4  ...             NaN  NaN           NaN           NaN           NaN  NaN   \n",
       "\n",
       "   kdp_5x5_10th  kdp_5x5_50th  kdp_5x5_90th  expected  \n",
       "0           NaN           NaN           NaN     0.254  \n",
       "1           NaN           NaN           NaN     0.254  \n",
       "2           NaN           NaN           NaN     0.254  \n",
       "3           NaN           NaN           NaN     0.254  \n",
       "4           NaN           NaN           NaN     0.254  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-layer deep stacked Bi-RNN, 64-256 hidden units, dense after each hidden stack, \n",
    "last layer at each time position vector is fed into a dense layer with 1 output and ReLU. Take mean of the predictions from the entire top layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "exclude the rain gauges with readings above 70mm.\n",
    "replace missing radar feature values with zero\n",
    "each timestamp as a component in the feature vector, sequential nature of the input is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_t['count_id'] = train_t.groupby('Id')['Id'].transform('count')\n",
    "#train_t['count_None_Ref'] = train_t[~np.isnan(train_t.Ref)].transform('count')\n",
    "train_nones = train_t[~np.isnan(train_t['ref'])]['id'].unique()\n",
    "train_full = train_t['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned = train_t[np.in1d(train_full, train_nones)].fillna(0.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = train_cleaned['id'].unique()\n",
    "train_ids = np.random.choice(ids,\n",
    "                               size=np.ceil(len(ids) * (9/10.)).astype(int),\n",
    "                               replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_t.loc[train_t['id'].isin(train_ids)]\n",
    "val_data = train_t.loc[~train_t['id'].isin(train_ids)]\n",
    "#[train.columns != 'expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>minutes_past</th>\n",
       "      <th>radardist_km</th>\n",
       "      <th>ref</th>\n",
       "      <th>ref_5x5_10th</th>\n",
       "      <th>ref_5x5_50th</th>\n",
       "      <th>ref_5x5_90th</th>\n",
       "      <th>refcomposite</th>\n",
       "      <th>refcomposite_5x5_10th</th>\n",
       "      <th>refcomposite_5x5_50th</th>\n",
       "      <th>...</th>\n",
       "      <th>rhohv_5x5_90th</th>\n",
       "      <th>zdr</th>\n",
       "      <th>zdr_5x5_10th</th>\n",
       "      <th>zdr_5x5_50th</th>\n",
       "      <th>zdr_5x5_90th</th>\n",
       "      <th>kdp</th>\n",
       "      <th>kdp_5x5_10th</th>\n",
       "      <th>kdp_5x5_50th</th>\n",
       "      <th>kdp_5x5_90th</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>10.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>-0.1250</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>1.059998</td>\n",
       "      <td>-1.410004</td>\n",
       "      <td>-0.350006</td>\n",
       "      <td>1.059998</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>31.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.005000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.409988</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>20.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>-0.0625</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.349991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.350006</td>\n",
       "      <td>1.759994</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.349991</td>\n",
       "      <td>-1.059998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.059998</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>-0.350006</td>\n",
       "      <td>-1.059998</td>\n",
       "      <td>-0.350006</td>\n",
       "      <td>1.759994</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  minutes_past  radardist_km   ref  ref_5x5_10th  ref_5x5_50th  \\\n",
       "6    2             1           2.0   9.0           5.0           7.5   \n",
       "7    2             6           2.0  26.5          22.5          25.5   \n",
       "8    2            11           2.0  21.5          15.5          20.5   \n",
       "9    2            16           2.0  18.0          14.0          17.5   \n",
       "10   2            21           2.0  24.5          16.5          21.0   \n",
       "\n",
       "    ref_5x5_90th  refcomposite  refcomposite_5x5_10th  refcomposite_5x5_50th  \\\n",
       "6           10.5          15.0                   10.5                   16.5   \n",
       "7           31.5          26.5                   26.5                   28.5   \n",
       "8           25.0          26.5                   23.5                   25.0   \n",
       "9           21.0          20.5                   18.0                   20.5   \n",
       "10          24.5          24.5                   21.0                   24.0   \n",
       "\n",
       "    ...  rhohv_5x5_90th     zdr  zdr_5x5_10th  zdr_5x5_50th  zdr_5x5_90th  \\\n",
       "6   ...        0.998333  0.3750       -0.1250        0.3125        0.8750   \n",
       "7   ...        1.005000  0.0625       -0.1875        0.2500        0.6875   \n",
       "8   ...        1.001667  0.3125       -0.0625        0.3125        0.6250   \n",
       "9   ...        1.001667  0.2500        0.1250        0.3750        0.6875   \n",
       "10  ...        0.998333  0.2500        0.0625        0.1875        0.5625   \n",
       "\n",
       "         kdp  kdp_5x5_10th  kdp_5x5_50th  kdp_5x5_90th  expected  \n",
       "6   1.059998     -1.410004     -0.350006      1.059998     1.016  \n",
       "7        NaN           NaN           NaN      1.409988     1.016  \n",
       "8   0.349991           NaN     -0.350006      1.759994     1.016  \n",
       "9   0.349991     -1.059998      0.000000      1.059998     1.016  \n",
       "10 -0.350006     -1.059998     -0.350006      1.759994     1.016  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.groupby('id')['expected'].last().values\n",
    "\n",
    "y_val = val_data.groupby('id')['expected'].last().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop('expected', 1).fillna(0.0)\n",
    "val_data = val_data.drop('expected', 1).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_arr = train_data.groupby('id').apply(lambda x: x.values.astype('float')).values\n",
    "val_data_arr = val_data.groupby('id').apply(lambda x: x.values.astype('float')).values\n",
    "\n",
    "seq_lens_train = train_data.groupby('id').size().values\n",
    "seq_lens_val = val_data.groupby('id').size().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 15, 12, 12,  7, 10, 10, 12, 12])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2837,), (2837,), (2360,), (2360,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_arr.shape, y_train.shape, val_data_arr.shape, y_val.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2837, 12, 23)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_arr.shape[0], train_data_arr[0].shape[0], train_data_arr[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = max([seq_lens_train.max(), seq_lens_val.max()])#choose the length of the sequence\n",
    "neurons = [64, 128, 256]\n",
    "n_in = train_data_arr[0].shape[1]#length of the feature vector of each element in a sequence\n",
    "n_out = 1#dim of the output feture vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pad = seq_pad(val_data_arr, n_in, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float32, shape=[None, seq_len , n_in])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "seq_length = tf.placeholder(dtype=tf.int16, shape=[None])\n",
    "\n",
    "with tf.variable_scope('lstm-rnn', initializer=tf.contrib.layers.variance_scaling_initializer()):\n",
    "    rnn_cell_1 = tf.contrib.rnn.LSTMCell(num_units=neurons[1])\n",
    "    rnn_cell_2 = tf.contrib.rnn.LSTMCell(num_units=neurons[1])\n",
    "    multi_layer_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell_1, rnn_cell_2])\n",
    "    outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32,\n",
    "                                        sequence_length=seq_length)\n",
    "    drop = tf.layers.dropout(inputs=states[1], rate=0.4)\n",
    "    y_pred = tf.layers.dense(drop, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "error = (y-y_pred)\n",
    "loss = tf.reduce_mean(tf.square(error), name=\"loss\")\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "minimizer = optimizer.minimize(loss, global_step=global_step)\n",
    "minimizer_val = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "mae = tf.reduce_mean(tf.abs(error))\n",
    "\n",
    "checkpoint_dir = './models/'\n",
    "best_ckpt_saver = BestCheckpointSaver(save_dir=checkpoint_dir, num_to_keep=3, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  Train loss: 17.0401, Train MAE: 1.88  Val MAE: 3.97\n",
      "   0  Train loss: 30.4458, Train MAE: 3.13  Val MAE: 3.97\n",
      "   0  Train loss: 18.9927, Train MAE: 2.45  Val MAE: 3.97\n",
      "   0  Train loss: 50.7828, Train MAE: 3.10  Val MAE: 3.97\n",
      "   0  Train loss: 55.7671, Train MAE: 3.70  Val MAE: 3.97\n",
      "   1  Train loss: 41.1146, Train MAE: 3.27  Val MAE: 4.03\n",
      "   1  Train loss: 41.2473, Train MAE: 2.93  Val MAE: 4.03\n",
      "   1  Train loss: 11.1018, Train MAE: 2.82  Val MAE: 4.03\n",
      "   1  Train loss: 8.5333, Train MAE: 2.16  Val MAE: 4.03\n",
      "   1  Train loss: 39.9714, Train MAE: 3.27  Val MAE: 4.03\n",
      "   2  Train loss: 29.2256, Train MAE: 3.19  Val MAE: 3.54\n",
      "   2  Train loss: 27.4878, Train MAE: 3.37  Val MAE: 3.54\n",
      "   2  Train loss: 14.0406, Train MAE: 2.57  Val MAE: 3.54\n",
      "   2  Train loss: 39.7434, Train MAE: 3.02  Val MAE: 3.54\n",
      "   2  Train loss: 22.0048, Train MAE: 2.86  Val MAE: 3.54\n",
      "   3  Train loss: 15.0573, Train MAE: 2.55  Val MAE: 3.67\n",
      "   3  Train loss: 58.9396, Train MAE: 2.88  Val MAE: 3.67\n",
      "   3  Train loss: 27.6798, Train MAE: 3.36  Val MAE: 3.67\n",
      "   3  Train loss: 30.3320, Train MAE: 3.23  Val MAE: 3.67\n",
      "   3  Train loss: 45.5163, Train MAE: 3.32  Val MAE: 3.67\n",
      "   4  Train loss: 24.0583, Train MAE: 3.13  Val MAE: 4.00\n",
      "   4  Train loss: 24.6374, Train MAE: 3.15  Val MAE: 4.00\n",
      "   4  Train loss: 24.1206, Train MAE: 2.77  Val MAE: 4.00\n",
      "   4  Train loss: 69.2199, Train MAE: 3.40  Val MAE: 4.00\n",
      "   4  Train loss: 10.8482, Train MAE: 2.48  Val MAE: 4.00\n",
      "   5  Train loss: 12.3987, Train MAE: 2.20  Val MAE: 3.45\n",
      "   5  Train loss: 23.9220, Train MAE: 2.96  Val MAE: 3.45\n",
      "   5  Train loss: 10.6681, Train MAE: 2.44  Val MAE: 3.45\n",
      "   5  Train loss: 62.6927, Train MAE: 3.90  Val MAE: 3.45\n",
      "   5  Train loss: 52.1528, Train MAE: 3.78  Val MAE: 3.45\n",
      "   6  Train loss: 20.4686, Train MAE: 2.96  Val MAE: 3.90\n",
      "   6  Train loss: 10.1794, Train MAE: 2.44  Val MAE: 3.90\n",
      "   6  Train loss: 114.9780, Train MAE: 4.55  Val MAE: 3.90\n",
      "   6  Train loss: 50.3025, Train MAE: 3.40  Val MAE: 3.90\n",
      "   6  Train loss: 39.2903, Train MAE: 3.86  Val MAE: 3.90\n",
      "   7  Train loss: 11.9720, Train MAE: 2.37  Val MAE: 3.80\n",
      "   7  Train loss: 8.5647, Train MAE: 2.03  Val MAE: 3.80\n",
      "   7  Train loss: 22.5333, Train MAE: 2.99  Val MAE: 3.80\n",
      "   7  Train loss: 14.5635, Train MAE: 2.91  Val MAE: 3.80\n",
      "   7  Train loss: 28.4300, Train MAE: 3.12  Val MAE: 3.80\n",
      "   8  Train loss: 10.2804, Train MAE: 2.43  Val MAE: 3.98\n",
      "   8  Train loss: 31.0449, Train MAE: 3.30  Val MAE: 3.98\n",
      "   8  Train loss: 22.7447, Train MAE: 2.49  Val MAE: 3.98\n",
      "   8  Train loss: 54.6773, Train MAE: 3.84  Val MAE: 3.98\n",
      "   8  Train loss: 22.5516, Train MAE: 3.16  Val MAE: 3.98\n",
      "   9  Train loss: 13.9364, Train MAE: 2.74  Val MAE: 3.93\n",
      "   9  Train loss: 9.9636, Train MAE: 2.10  Val MAE: 3.93\n",
      "   9  Train loss: 14.7693, Train MAE: 2.20  Val MAE: 3.93\n",
      "   9  Train loss: 72.3280, Train MAE: 3.25  Val MAE: 3.93\n",
      "   9  Train loss: 38.1931, Train MAE: 3.32  Val MAE: 3.93\n",
      "  10  Train loss: 22.8723, Train MAE: 3.36  Val MAE: 4.48\n",
      "  10  Train loss: 53.1000, Train MAE: 3.68  Val MAE: 4.48\n",
      "  10  Train loss: 12.6827, Train MAE: 2.28  Val MAE: 4.48\n",
      "  10  Train loss: 49.4061, Train MAE: 3.39  Val MAE: 4.48\n",
      "  10  Train loss: 19.3717, Train MAE: 2.90  Val MAE: 4.48\n",
      "  11  Train loss: 62.0810, Train MAE: 3.43  Val MAE: 3.96\n",
      "  11  Train loss: 39.4831, Train MAE: 3.36  Val MAE: 3.96\n",
      "  11  Train loss: 33.2514, Train MAE: 2.81  Val MAE: 3.96\n",
      "  11  Train loss: 52.9088, Train MAE: 3.22  Val MAE: 3.96\n",
      "  11  Train loss: 46.2672, Train MAE: 3.67  Val MAE: 3.96\n",
      "  12  Train loss: 17.8765, Train MAE: 3.01  Val MAE: 4.07\n",
      "  12  Train loss: 17.7192, Train MAE: 2.64  Val MAE: 4.07\n",
      "  12  Train loss: 84.3497, Train MAE: 3.51  Val MAE: 4.07\n",
      "  12  Train loss: 35.5774, Train MAE: 3.07  Val MAE: 4.07\n",
      "  12  Train loss: 26.5101, Train MAE: 3.20  Val MAE: 4.07\n",
      "  13  Train loss: 12.7249, Train MAE: 2.59  Val MAE: 3.94\n",
      "  13  Train loss: 27.1148, Train MAE: 3.63  Val MAE: 3.94\n",
      "  13  Train loss: 55.9337, Train MAE: 3.27  Val MAE: 3.94\n",
      "  13  Train loss: 25.4015, Train MAE: 3.02  Val MAE: 3.94\n",
      "  13  Train loss: 60.2883, Train MAE: 3.32  Val MAE: 3.94\n",
      "  14  Train loss: 31.2780, Train MAE: 3.17  Val MAE: 3.82\n",
      "  14  Train loss: 15.0528, Train MAE: 2.78  Val MAE: 3.82\n",
      "  14  Train loss: 17.7562, Train MAE: 2.68  Val MAE: 3.82\n",
      "  14  Train loss: 12.5110, Train MAE: 2.27  Val MAE: 3.82\n",
      "  14  Train loss: 24.3478, Train MAE: 2.94  Val MAE: 3.82\n",
      "  15  Train loss: 40.3956, Train MAE: 3.66  Val MAE: 3.80\n",
      "  15  Train loss: 17.1382, Train MAE: 3.21  Val MAE: 3.80\n",
      "  15  Train loss: 23.7194, Train MAE: 3.14  Val MAE: 3.80\n",
      "  15  Train loss: 45.5113, Train MAE: 3.24  Val MAE: 3.80\n",
      "  15  Train loss: 13.2734, Train MAE: 2.27  Val MAE: 3.80\n",
      "  16  Train loss: 82.3754, Train MAE: 3.57  Val MAE: 3.55\n",
      "  16  Train loss: 16.4765, Train MAE: 2.89  Val MAE: 3.55\n",
      "  16  Train loss: 12.5183, Train MAE: 2.76  Val MAE: 3.55\n",
      "  16  Train loss: 47.6336, Train MAE: 3.06  Val MAE: 3.55\n",
      "  16  Train loss: 51.5400, Train MAE: 3.47  Val MAE: 3.55\n",
      "  17  Train loss: 65.4468, Train MAE: 3.40  Val MAE: 3.55\n",
      "  17  Train loss: 58.9042, Train MAE: 3.19  Val MAE: 3.55\n",
      "  17  Train loss: 15.9287, Train MAE: 2.78  Val MAE: 3.55\n",
      "  17  Train loss: 27.6092, Train MAE: 3.36  Val MAE: 3.55\n"
     ]
    }
   ],
   "source": [
    "n_samples = train_data_arr.shape[0]\n",
    "n_epochs = 30\n",
    "batch_size = len(train_data_arr) / 25\n",
    "#printbatch_size\n",
    "loss_hist = {'mae': {'train': [], 'val': []}, 'loss':{'train':[], 'val':[]}}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_idx = np.random.permutation(np.arange(n_samples))\n",
    "        train_loss_list = []\n",
    "        train_mae_list = []\n",
    "        for batch_i in range(int(n_samples // batch_size)):\n",
    "            batch_idx = rand_idx[int(batch_i*batch_size):int((batch_i+1)*batch_size)]\n",
    "            X_batch = seq_pad(train_data_arr[batch_idx], n_in, seq_len)\n",
    "            y_batch = y_train[batch_idx]\n",
    "            len_batch = seq_lens_train[batch_idx]\n",
    "            loss_train, _, step = sess.run(\n",
    "                    [loss, minimizer, global_step],\n",
    "                    feed_dict={X: X_batch, seq_length: len_batch, y: y_batch})\n",
    "            mae_train = mae.eval(feed_dict={X: X_batch, seq_length: len_batch, y: y_batch})\n",
    "            train_mae_list.append(float(mae_train))\n",
    "            train_loss_list.append(loss_train)\n",
    "            if batch_i % 5 == 0:\n",
    "                print(\"{:4d}  Train loss: {:.4f}, Train MAE: {:.2f}  Val MAE: {:.2f}\".format(\n",
    "                epoch, loss_train, mae_train, mae_val))\n",
    "        loss_hist['loss']['train'].append(float(np.array(train_loss_list).mean()))\n",
    "        loss_hist['mae']['train'].append(float(np.array(train_mae_list).mean()))\n",
    "        loss_val, step = sess.run(\n",
    "                    [loss, global_step],\n",
    "                    feed_dict={X: val_pad, seq_length: seq_lens_val, y: y_val})\n",
    "        loss_hist['loss']['val'].append(loss_val)\n",
    "        \n",
    "        mae_val = mae.eval(feed_dict={X: val_pad, seq_length: seq_lens_val, y: y_val})\n",
    "        loss_hist['mae']['val'].append(float(mae_val))\n",
    "        \n",
    "        \n",
    "        best_ckpt_saver.handle(loss_val, sess, global_step)\n",
    "            #saver.save(sess, \"./models/model1.ckpt\")\n",
    "        #final_pred = y_pred.eval({X: val_pad, seq_length: seq_lens_val, y: y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "#history of accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(loss_hist['loss']['train'])\n",
    "plt.plot(loss_hist['loss']['val'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "#history of accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(loss_hist['mae']['train'])\n",
    "plt.plot(loss_hist['mae']['val'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/best.ckpt-5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b6ad3579ed8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckmate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_maximum_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1725\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1726\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, checkmate.get_best_checkpoint(checkpoint_dir, select_maximum_value=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = y_pred.eval({X: test_pad, seq_length: test_lens, y: y_test})\n",
    "np.abs((pred - y_test)).argmin()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
